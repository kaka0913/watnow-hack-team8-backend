import os
import time
import math
import json
import requests
import pygeohash as pgh
from dotenv import load_dotenv
from supabase import create_client, Client
from shapely.geometry import box, Point

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# --- è¨­å®šé …ç›® ---
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_ANON_KEY")
GOOGLE_MAPS_API_KEY = os.environ.get("GOOGLE_MAPS_API_KEY")

GEOHASH_PRECISION = 6 # ã‚°ãƒªãƒƒãƒ‰ã®ç²¾åº¦
MIN_RATING = 3.5
MAX_RETRIES = 3  # APIå¤±æ•—æ™‚ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
CHUNK_SIZE = 10  # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹Geohashã®æ•°
PROGRESS_FILE = 'pois_migration_progress.json'  # é€²æ—ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«

# æ•£æ­©ãŒæ¥½ã—ããªã‚‹ã‚¹ãƒãƒƒãƒˆã®ã‚«ãƒ†ã‚´ãƒª
POI_TYPES = [
    'cafe', 'park', 'tourist_attraction', 'art_gallery', 'book_store',
    'bakery', 'store', 'home_goods_store', 'museum',
    'shrine', 'temple', 'florist', 'library', 'riverside_park'
]

# æ²³å·æ•·åˆ¤å®šç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
RIVERSIDE_KEYWORDS = [
    'æ²³å·æ•·', 'å·åŸ', 'æ²³åŸ', 'æ²³å·', 'å ¤é˜²', 'å·è¾º', 'æ°´è¾º', 
    'ãƒªãƒãƒ¼ã‚µã‚¤ãƒ‰', 'riverside', 'å·æ²¿ã„', 'æ²³å²¸'
]

# --- ã‚¨ãƒªã‚¢å®šç¾© (4äººåˆ†æ‹…ç”¨) ---

# ã€KABUã•ã‚“ã®æ‹…å½“ã‚¨ãƒªã‚¢ã€‘äº¬éƒ½ä¸­å¿ƒéƒ¨ï¼‹åµå±± - ç´°åˆ†åŒ–ç‰ˆ (å„ã‚¨ãƒªã‚¢ç´„30åˆ†å‡¦ç†)

# KABU_1: å››æ¡çƒä¸¸ã‚¨ãƒªã‚¢ä¸­å¿ƒéƒ¨
BOUNDING_BOXES_KABU_1 = [
    {'name': 'A1-1: Shijo_Karasuma_Core', 'min_lat': 35.002, 'max_lat': 35.005, 'min_lon': 135.758, 'max_lon': 135.765},
    {'name': 'A1-2: Karasuma_Gojo', 'min_lat': 35.000, 'max_lat': 35.003, 'min_lon': 135.760, 'max_lon': 135.768},
]

# KABU_2: æ²³åŸç”ºã‚¨ãƒªã‚¢
BOUNDING_BOXES_KABU_2 = [
    {'name': 'A2-1: Kawaramachi_Shijo', 'min_lat': 35.003, 'max_lat': 35.006, 'min_lon': 135.765, 'max_lon': 135.772},
    {'name': 'A2-2: Kawaramachi_Sanjo', 'min_lat': 35.005, 'max_lat': 35.008, 'min_lon': 135.767, 'max_lon': 135.775},
]

# KABU_3: ç¥‡åœ’ã‚¨ãƒªã‚¢è¥¿éƒ¨
BOUNDING_BOXES_KABU_3 = [
    {'name': 'A3-1: Gion_West', 'min_lat': 34.995, 'max_lat': 35.000, 'min_lon': 135.774, 'max_lon': 135.781},
    {'name': 'A3-2: Gion_Shirakawa', 'min_lat': 35.000, 'max_lat': 35.004, 'min_lon': 135.774, 'max_lon': 135.780},
]

# KABU_4: ç¥‡åœ’ãƒ»æ±å±±ã‚¨ãƒªã‚¢æ±éƒ¨
BOUNDING_BOXES_KABU_4 = [
    {'name': 'A4-1: Higashiyama_South', 'min_lat': 34.992, 'max_lat': 34.998, 'min_lon': 135.780, 'max_lon': 135.788},
    {'name': 'A4-2: Higashiyama_North', 'min_lat': 34.998, 'max_lat': 35.004, 'min_lon': 135.780, 'max_lon': 135.788},
]

# KABU_5: æ±å±±å±±éº“ã‚¨ãƒªã‚¢
BOUNDING_BOXES_KABU_5 = [
    {'name': 'A5-1: Maruyama_Yasaka', 'min_lat': 35.003, 'max_lat': 35.008, 'min_lon': 135.780, 'max_lon': 135.788},
    {'name': 'A5-2: Chionin_Shoren', 'min_lat': 35.006, 'max_lat': 35.008, 'min_lon': 135.774, 'max_lon': 135.782},
]

# KABU_6: åµå±±ã‚¨ãƒªã‚¢è¥¿éƒ¨ï¼ˆç«¹æ—ãƒ»å¤©é¾å¯ºï¼‰
BOUNDING_BOXES_KABU_6 = [
    {'name': 'A6-1: Arashiyama_Bamboo', 'min_lat': 35.015, 'max_lat': 35.018, 'min_lon': 135.665, 'max_lon': 135.672},
    {'name': 'A6-2: Tenryuji_Area', 'min_lat': 35.013, 'max_lat': 35.017, 'min_lon': 135.672, 'max_lon': 135.678},
]

# KABU_7: åµå±±ã‚¨ãƒªã‚¢ä¸­å¤®ï¼ˆæ¸¡æœˆæ©‹å‘¨è¾ºï¼‰
BOUNDING_BOXES_KABU_7 = [
    {'name': 'A7-1: Togetsukyo_Bridge', 'min_lat': 35.010, 'max_lat': 35.014, 'min_lon': 135.675, 'max_lon': 135.682},
    {'name': 'A7-2: Arashiyama_Station', 'min_lat': 35.012, 'max_lat': 35.016, 'min_lon': 135.678, 'max_lon': 135.685},
]

# KABU_8: åµ¯å³¨é‡ã‚¨ãƒªã‚¢åŒ—éƒ¨
BOUNDING_BOXES_KABU_8 = [
    {'name': 'A8-1: Sagano_North', 'min_lat': 35.017, 'max_lat': 35.020, 'min_lon': 135.668, 'max_lon': 135.675},
    {'name': 'A8-2: Adashino_Nenbutsuji', 'min_lat': 35.018, 'max_lat': 35.020, 'min_lon': 135.675, 'max_lon': 135.682},
]

# â˜…â˜…â˜… å®Ÿè¡Œæ™‚ã«ä½¿ç”¨ã™ã‚‹ã‚¨ãƒªã‚¢ã‚’é¸æŠã—ã¦ãã ã•ã„ â˜…â˜…â˜…
# å‡¦ç†é †åºã®æ¨å¥¨ï¼šKABU_1 â†’ KABU_2 â†’ KABU_3 â†’ KABU_4 â†’ KABU_5 â†’ KABU_6 â†’ KABU_7 â†’ KABU_8

# çµ±åˆã‚¨ãƒªã‚¢ï¼ˆå…¨ã¦å®Ÿè¡Œã™ã‚‹å ´åˆï¼‰
BOUNDING_BOXES_KABU_ALL = (
    BOUNDING_BOXES_KABU_1 + BOUNDING_BOXES_KABU_2 + BOUNDING_BOXES_KABU_3 + BOUNDING_BOXES_KABU_4 +
    BOUNDING_BOXES_KABU_5 + BOUNDING_BOXES_KABU_6 + BOUNDING_BOXES_KABU_7 + BOUNDING_BOXES_KABU_8
)

# ã€KIMã•ã‚“ã®æ‹…å½“ã‚¨ãƒªã‚¢ã€‘äº¬éƒ½å—éƒ¨ãƒ»åŒ—éƒ¨ï¼‹æ»‹è³€è¥¿éƒ¨ (ç´„320 Geohash)
BOUNDING_BOXES_KIM = [
    {'name': 'B: Kyoto_Gojo_Station', 'min_lat': 34.983, 'max_lat': 35.000, 'min_lon': 135.755, 'max_lon': 135.768},
    {'name': 'B: Fushimi_Inari', 'min_lat': 34.965, 'max_lat': 34.975, 'min_lon': 135.770, 'max_lon': 135.780},
    {'name': 'B: Kinkakuji_Kitano', 'min_lat': 35.025, 'max_lat': 35.039, 'min_lon': 135.725, 'max_lon': 135.745},
    {'name': 'B: Shiga_Otsu', 'min_lat': 35.000, 'max_lat': 35.025, 'min_lon': 135.850, 'max_lon': 135.880},
]

# ã€KOSUKEã•ã‚“ã®æ‹…å½“ã‚¨ãƒªã‚¢ã€‘å¤§é˜ªã‚­ã‚¿ï¼‹ãƒŸãƒŠãƒŸ (ç´„350 Geohash)
BOUNDING_BOXES_KOSUKE = [
    {'name': 'C: Osaka_Kita_Umeda', 'min_lat': 34.695, 'max_lat': 34.710, 'min_lon': 135.490, 'max_lon': 135.510},
    {'name': 'C: Osaka_Minami_Namba', 'min_lat': 34.660, 'max_lat': 34.675, 'min_lon': 135.495, 'max_lon': 135.515},
]

# ã€RIHOã•ã‚“ã®æ‹…å½“ã‚¨ãƒªã‚¢ã€‘å¤§é˜ªãã®ä»–ï¼‹æ»‹è³€æ±éƒ¨ (ç´„250 Geohash)
BOUNDING_BOXES_RIHO = [
    {'name': 'D: Osaka_Tennoji_Shinsekai', 'min_lat': 34.645, 'max_lat': 34.658, 'min_lon': 135.505, 'max_lon': 135.520},
    {'name': 'D: Osaka_Castle', 'min_lat': 34.682, 'max_lat': 34.692, 'min_lon': 135.520, 'max_lon': 135.535},
    {'name': 'D: Shiga_Kusatsu', 'min_lat': 35.010, 'max_lat': 35.035, 'min_lon': 135.940, 'max_lon': 135.970},
]

# ===================================================================
# KABUã•ã‚“å‘ã‘ä½¿ç”¨æ–¹æ³•:
# 1. ä¸Šè¨˜ã®TARGET_BOUNDING_BOXESã§1ã¤ãšã¤ã‚¨ãƒªã‚¢ã‚’é¸æŠ
# 2. æ¨å¥¨é †åº: KABU_1 â†’ KABU_2 â†’ KABU_3 â†’ KABU_4 â†’ KABU_5 â†’ KABU_6 â†’ KABU_7 â†’ KABU_8
# 3. å„ã‚¨ãƒªã‚¢ã¯ç´„30åˆ†ã§å®Œäº†äºˆå®šï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ³ã«ã‚ˆã‚Šå¤‰å‹•ï¼‰
# 4. é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè‡ªå‹•ä¿å­˜ã•ã‚Œã‚‹ãŸã‚ã€ä¸­æ–­ã—ã¦ã‚‚ç¶šãã‹ã‚‰å®Ÿè¡Œå¯èƒ½
# ===================================================================


# â˜…â˜…â˜… è‡ªåˆ†ã®æ‹…å½“ã«åˆã‚ã›ã¦ã€ä»¥ä¸‹ã®1è¡Œã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ â˜…â˜…â˜…

# KABUã•ã‚“ç”¨ï¼ˆç´°åˆ†åŒ–ã‚¨ãƒªã‚¢ - 1ã¤ãšã¤å®Ÿè¡Œæ¨å¥¨ï¼‰
TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_1  # å››æ¡çƒä¸¸ã‚¨ãƒªã‚¢ä¸­å¿ƒéƒ¨ï¼ˆç´„30åˆ†ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_2  # æ²³åŸç”ºã‚¨ãƒªã‚¢ï¼ˆç´„30åˆ†ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_3  # ç¥‡åœ’ã‚¨ãƒªã‚¢è¥¿éƒ¨ï¼ˆç´„30åˆ†ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_4  # ç¥‡åœ’ãƒ»æ±å±±ã‚¨ãƒªã‚¢æ±éƒ¨ï¼ˆç´„30åˆ†ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_5  # æ±å±±å±±éº“ã‚¨ãƒªã‚¢ï¼ˆç´„30åˆ†ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_6  # åµå±±ã‚¨ãƒªã‚¢è¥¿éƒ¨ï¼ˆç´„30åˆ†ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_7  # åµå±±ã‚¨ãƒªã‚¢ä¸­å¤®ï¼ˆç´„30åˆ†ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_8  # åµ¯å³¨é‡ã‚¨ãƒªã‚¢åŒ—éƒ¨ï¼ˆç´„30åˆ†ï¼‰

# å…¨ã‚¨ãƒªã‚¢ã‚’ä¸€åº¦ã«å®Ÿè¡Œã™ã‚‹å ´åˆï¼ˆéæ¨å¥¨ï¼šæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KABU_ALL

# ä»–ã®ãƒ¡ãƒ³ãƒãƒ¼ç”¨
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KIM
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_KOSUKE
# TARGET_BOUNDING_BOXES = BOUNDING_BOXES_RIHO


# --- ã‚¹ã‚¯ãƒªãƒ—ãƒˆæœ¬ä½“ ---

def save_progress(processed_geohashes, total_geohashes):
    """é€²æ—ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    progress_data = {
        'processed_geohashes': list(processed_geohashes),
        'total_count': len(total_geohashes),
        'processed_count': len(processed_geohashes),
        'timestamp': time.time()
    }
    try:
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        print(f"é€²æ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ ({len(processed_geohashes)}/{len(total_geohashes)})")
    except Exception as e:
        print(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def load_progress():
    """é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‡¦ç†æ¸ˆã¿Geohashã‚’èª­ã¿è¾¼ã¿"""
    try:
        if os.path.exists(PROGRESS_FILE):
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            processed = set(progress_data.get('processed_geohashes', []))
            print(f"å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(processed)}ä»¶å‡¦ç†æ¸ˆã¿")
            return processed
    except Exception as e:
        print(f"é€²æ—èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return set()

def clear_progress():
    """é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
    try:
        if os.path.exists(PROGRESS_FILE):
            os.remove(PROGRESS_FILE)
            print("é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

def get_geohashes_in_all_boxes(bounding_boxes, precision):
    """æŒ‡å®šã—ãŸè¤‡æ•°ã®çŸ©å½¢ç¯„å›²å†…ã®Geohashã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹"""
    all_geohashes = set()
    lat_step = 0.005 # ç²¾åº¦6ã«åˆã‚ã›ã¦ã‚¹ãƒ†ãƒƒãƒ—ã‚’èª¿æ•´
    lon_step = 0.005

    for bbox in bounding_boxes:
        print(f"  - ã‚¨ãƒªã‚¢ '{bbox['name']}' ã®Geohashã‚’è¨ˆç®—ä¸­...")
        lat = bbox['min_lat']
        while lat < bbox['max_lat']:
            lon = bbox['min_lon']
            while lon < bbox['max_lon']:
                all_geohashes.add(pgh.encode(lat, lon, precision))
                lon += lon_step
            lat += lat_step
    return list(all_geohashes)

def is_riverside_park(place_name, place_types):
    """å…¬åœ’ãŒæ²³å·æ•·ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹"""
    # åå‰ã«æ²³å·æ•·é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    name_lower = place_name.lower()
    for keyword in RIVERSIDE_KEYWORDS:
        if keyword.lower() in name_lower:
            return True
    return False

def ensure_grid_cells_exist(supabase: Client, geohashes):
    """DBã«grid_cellãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ãªã‘ã‚Œã°ä½œæˆã™ã‚‹ã€‚geohash->idã®è¾æ›¸ã‚’è¿”ã™ã€‚"""
    print("ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ã®æº–å‚™ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # æ—¢å­˜ã®grid_cellsã‚’ç¢ºèª
    try:
        existing_response = supabase.table('grid_cells').select('id, geohash').execute()
        print(f"ğŸ” æ—¢å­˜ã®grid_cells: {len(existing_response.data)}ä»¶")
    except Exception as e:
        print(f"âŒ grid_cellsãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    cells_to_insert = []
    for geohash in geohashes:
        bbox = pgh.get_bounding_box(geohash)
        cell_polygon = box(bbox.min_lon, bbox.min_lat, bbox.max_lon, bbox.max_lat)
        cells_to_insert.append({
            'geohash': geohash,
            'geometry': f"SRID=4326;{cell_polygon.wkt}"
        })
    
    try:
        # geohashãŒé‡è¤‡ã—ãŸå ´åˆã¯ç„¡è¦–ã™ã‚‹ (upsert)
        supabase.table('grid_cells').upsert(cells_to_insert, on_conflict='geohash').execute()
        print("âœ… ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
        return None

    # ä½œæˆ/ç¢ºèªã—ãŸã‚»ãƒ«ã®æƒ…å ±ã‚’DBã‹ã‚‰å–å¾—ã—ã¦è¾æ›¸ã‚’ä½œæˆ
    try:
        response = supabase.table('grid_cells').select('id, geohash').in_('geohash', geohashes).execute()
        geohash_to_id_map = {cell['geohash']: cell['id'] for cell in response.data}
        
        # å…¨ã¦ã®geohashã«å¯¾ã—ã¦grid_cell_idãŒå–å¾—ã§ããŸã‹ç¢ºèª
        missing_geohashes = [gh for gh in geohashes if gh not in geohash_to_id_map]
        if missing_geohashes:
            print(f"âš ï¸ è­¦å‘Š: {len(missing_geohashes)}å€‹ã®geohashã§grid_cell_idãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            print(f"   ä¾‹: {missing_geohashes[:3]}")
        
        print(f"ğŸ“¦ geohash->grid_cell_id ãƒãƒƒãƒ”ãƒ³ã‚°: {len(geohash_to_id_map)}ä»¶")
        return geohash_to_id_map
    except Exception as e:
        print(f"âŒ grid_cellå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def fetch_places(lat, lon, radius, poi_type):
    """Google Places API (New)ã‹ã‚‰ã‚¹ãƒãƒƒãƒˆæƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    all_places = []
    url = "https://places.googleapis.com/v1/places:searchNearby"
    
    # POI_TYPEã‚’Places API (New)ã®å½¢å¼ã«å¤‰æ›
    type_mapping = {
        'cafe': 'cafe',
        'park': 'park',
        'tourist_attraction': 'tourist_attraction',
        'art_gallery': 'art_gallery',
        'book_store': 'book_store',
        'bakery': 'bakery',
        'store': 'store',
        'home_goods_store': 'home_goods_store',
        'museum': 'museum',
        'shrine': 'hindu_temple',  # shrineã«æœ€ã‚‚è¿‘ã„ã‚¿ã‚¤ãƒ—
        'temple': 'hindu_temple',
        'florist': 'florist',
        'library': 'library',
        'riverside_park': 'park'  # riverside_parkã¯å¾Œã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    }
    
    included_type = type_mapping.get(poi_type, poi_type)
    
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': GOOGLE_MAPS_API_KEY,
        'X-Goog-FieldMask': 'places.id,places.displayName,places.location,places.rating,places.types,places.primaryType'
    }
    
    data = {
        'locationRestriction': {
            'circle': {
                'center': {
                    'latitude': lat,
                    'longitude': lon
                },
                'radius': radius
            }
        },
        'includedTypes': [included_type],
        'maxResultCount': 20,
        'languageCode': 'ja'
    }
    
    retry_count = 0
    while retry_count <= MAX_RETRIES:
        try:
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()
            results = response.json()
            
            # æ–°ã—ã„APIã®çµæœã‚’å¤ã„å½¢å¼ã«å¤‰æ›
            places = results.get('places', [])
            for place in places:
                converted_place = {
                    'place_id': place.get('id', ''),
                    'name': place.get('displayName', {}).get('text', ''),
                    'rating': place.get('rating', 0),
                    'types': place.get('types', []),
                    'geometry': {
                        'location': {
                            'lat': place.get('location', {}).get('latitude', 0),
                            'lng': place.get('location', {}).get('longitude', 0)
                        }
                    }
                }
                all_places.append(converted_place)
            
            break  # æ–°ã—ã„APIã¯ãƒšãƒ¼ã‚¸ãƒ³ã‚°ãŒç•°ãªã‚‹ãŸã‚ã€ä¸€åº¦ã§å–å¾—
                
        except requests.exceptions.RequestException as e:
            retry_count += 1
            if retry_count <= MAX_RETRIES:
                wait_time = 2 ** retry_count  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                print(f"    -> APIã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ{retry_count}/{MAX_RETRIES}): {e}")
                print(f"    -> {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(wait_time)
            else:
                print(f"    -> æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ: {e}")
                break
        except Exception as e:
            print(f"    -> APIè­¦å‘Š: {str(e)}")
            break
            
    return all_places

def populate_pois_chunk(supabase: Client, geohash_chunk, geohash_to_id_map, processed_geohashes, all_geohashes):
    """Geohashãƒãƒ£ãƒ³ã‚¯ã«å¯¾å¿œã™ã‚‹POIã‚’DBã«ä¿å­˜ã™ã‚‹"""
    print(f"{len(geohash_chunk)}å€‹ã®Geohashã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    
    for i, geohash in enumerate(geohash_chunk):
        try:
            print(f"  - Geohash {i+1}/{len(geohash_chunk)}: {geohash} ã‚’å‡¦ç†ä¸­...")
            lat, lon = pgh.decode(geohash)
            (lat_err, lon_err) = pgh.decode_exactly(geohash)[2:4]
            radius = ((lat_err * 111000)**2 + (lon_err * 111000)**2)**0.5

            found_place_ids = set()
            pois_to_insert = []
            park_places = []  # parkã®çµæœã‚’ä¿å­˜
            
            for poi_type in POI_TYPES:
                # riverside_parkã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆparkã®çµæœã‚’å¾Œã§å‡¦ç†ï¼‰
                if poi_type == 'riverside_park':
                    continue
                    
                places = fetch_places(lat, lon, radius, poi_type)
                
                # parkã®å ´åˆã¯çµæœã‚’ä¿å­˜
                if poi_type == 'park':
                    park_places = places
                
                for place in places:
                    place_id = place.get('place_id')
                    rating = place.get('rating', 0)

                    # parkã®å ´åˆã¯3.0ä»¥ä¸Šã€ãã®ä»–ã¯3.5ä»¥ä¸Š
                    min_rating = 3.0 if poi_type == 'park' else MIN_RATING
                    
                    if place_id and place_id not in found_place_ids and rating >= min_rating:
                        place_name = place['name']
                        place_types = place.get('types', [])
                        
                        # parkã‚¿ã‚¤ãƒ—ã§æ²³å·æ•·ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå¾Œã§riverside_parkã¨ã—ã¦å‡¦ç†ï¼‰
                        if poi_type == 'park' and is_riverside_park(place_name, place_types):
                            continue
                        
                        found_place_ids.add(place_id)
                        loc = place['geometry']['location']
                        point = Point(loc['lng'], loc['lat'])
                        
                        # ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®šï¼ˆæœ€åˆã®è¦ç´ ã¾ãŸã¯ poi_typeï¼‰
                        main_category = place_types[0] if place_types else poi_type
                        
                        pois_to_insert.append({
                            'id': place_id,
                            'name': place_name,
                            'location': f"SRID=4326;{point.wkt}",
                            'category': main_category,
                            'rate': rating,
                            'grid_cell_id': geohash_to_id_map.get(geohash)
                        })
            
            # parkã®çµæœã‹ã‚‰æ²³å·æ•·ã‚’æŠ½å‡ºã—ã¦riverside_parkã¨ã—ã¦å‡¦ç†
            for place in park_places:
                place_id = place.get('place_id')
                rating = place.get('rating', 0)

                # æ²³å·æ•·ã‚‚å…¬åœ’ãªã®ã§3.0ä»¥ä¸Šã§åˆ¤å®š
                if place_id and place_id not in found_place_ids and rating >= 3.0:
                    place_name = place['name']
                    place_types = place.get('types', [])
                    
                    # æ²³å·æ•·ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
                    if is_riverside_park(place_name, place_types):
                        found_place_ids.add(place_id)
                        loc = place['geometry']['location']
                        point = Point(loc['lng'], loc['lat'])
                        
                        # riverside_parkã‚’å„ªå…ˆã‚«ãƒ†ã‚´ãƒªã¨ã—ã¦è¨­å®š
                        pois_to_insert.append({
                            'id': place_id,
                            'name': place_name,
                            'location': f"SRID=4326;{point.wkt}",
                            'category': 'riverside_park',
                            'rate': rating,
                            'grid_cell_id': geohash_to_id_map.get(geohash)
                        })

            if pois_to_insert:
                try:
                    # POIä¿å­˜å‰ã«grid_cell_idã®æ¤œè¨¼
                    grid_cell_id = geohash_to_id_map.get(geohash)
                    if not grid_cell_id:
                        print(f"    âŒ ã‚¨ãƒ©ãƒ¼: geohash {geohash} ã«å¯¾ã™ã‚‹grid_cell_idãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        continue
                    
                    # ä¿å­˜ç›´å‰ã«grid_cellã®å­˜åœ¨ã‚’å†ç¢ºèª
                    verify_response = supabase.table('grid_cells').select('id').eq('id', grid_cell_id).execute()
                    if not verify_response.data:
                        print(f"    âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: grid_cell ID {grid_cell_id} ãŒä¿å­˜ç›´å‰ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        continue
                    
                    print(f"    ğŸ’¾ {len(pois_to_insert)}ä»¶ã®POIã‚’DBã«ä¿å­˜ä¸­ï¼ˆgrid_cell_id: {grid_cell_id}ï¼‰...")
                    supabase.table('pois').upsert(pois_to_insert, on_conflict='id').execute()
                    print(f"    âœ… {len(pois_to_insert)}ä»¶ã®POIã‚’DBã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    print(f"    âŒ POIæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}")
                    print(f"       Geohash: {geohash}, grid_cell_id: {geohash_to_id_map.get(geohash)}")
                    continue  # ã“ã®Geohashã¯å¤±æ•—ã¨ã—ã¦æ‰±ã†ãŒã€æ¬¡ã«é€²ã‚€
            
            # æˆåŠŸã—ãŸGeohashã‚’è¨˜éŒ²
            processed_geohashes.add(geohash)
            
            # 5å€‹å‡¦ç†ã™ã‚‹ã”ã¨ã«é€²æ—ä¿å­˜
            if len(processed_geohashes) % 5 == 0:
                save_progress(processed_geohashes, all_geohashes)
            
            time.sleep(1) # APIã®éå‰°ãªé€£ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é˜²ããŸã‚ã®ã‚¦ã‚§ã‚¤ãƒˆ
            
        except Exception as e:
            print(f"    -> Geohash {geohash} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue  # ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã‚‚æ¬¡ã®Geohashã«é€²ã‚€

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if 'TARGET_BOUNDING_BOXES' not in globals():
        print("\nâ˜…â˜…â˜… ã‚¨ãƒ©ãƒ¼ â˜…â˜…â˜…")
        print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¸Šéƒ¨ã®ã€Œè‡ªåˆ†ã®æ‹…å½“ã«åˆã‚ã›ã¦ã€ä»¥ä¸‹ã®1è¡Œã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦ãã ã•ã„ã€ã®éƒ¨åˆ†ã§ã€")
        print("å®Ÿè¡Œã—ãŸã„ã‚¨ãƒªã‚¢ã®è¡Œã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤ã—ã¦ãã ã•ã„ã€‚ (ä¾‹: TARGET_BOUNDING_BOXES = BOUNDING_BOXES_A)")
        return

    if not all([SUPABASE_URL, SUPABASE_KEY, GOOGLE_MAPS_API_KEY]):
        print("ã‚¨ãƒ©ãƒ¼: .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ãè¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    # é€²æ—ç¢ºèª
    processed_geohashes = load_progress()
    restart = input("\nå‰å›ã®ç¶šãã‹ã‚‰å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
    if restart != 'y':
        processed_geohashes.clear()
        clear_progress()
        print("æœ€åˆã‹ã‚‰å®Ÿè¡Œã—ã¾ã™ã€‚")
    
    print("å…¨å¯¾è±¡ã‚¨ãƒªã‚¢ã®Geohashã‚’è¨ˆç®—ä¸­...")
    all_geohashes = get_geohashes_in_all_boxes(TARGET_BOUNDING_BOXES, GEOHASH_PRECISION)
    print(f"åˆè¨ˆ {len(all_geohashes)}å€‹ã®GeohashãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    
    # æœªå‡¦ç†ã®Geohashã‚’æŠ½å‡º
    remaining_geohashes = [gh for gh in all_geohashes if gh not in processed_geohashes]
    print(f"æœªå‡¦ç†: {len(remaining_geohashes)}å€‹ã®Geohash")
    
    if not remaining_geohashes:
        print("ã™ã¹ã¦ã®GeohashãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚")
        return
    
    # ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ã‚’æº–å‚™ã—ã€geohash->idã®å¯¾å¿œè¾æ›¸ã‚’å–å¾—
    print("\nğŸ”§ ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«æº–å‚™ä¸­...")
    geohash_to_id_map = ensure_grid_cells_exist(supabase, all_geohashes)
    if not geohash_to_id_map:
        print("âŒ ã‚°ãƒªãƒƒãƒ‰ã‚»ãƒ«ã®æº–å‚™ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        return
    
    print(f"âœ… grid_cellãƒãƒƒãƒ”ãƒ³ã‚°æº–å‚™å®Œäº†: {len(geohash_to_id_map)}ä»¶")

    # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†
    try:
        for i in range(0, len(remaining_geohashes), CHUNK_SIZE):
            chunk = remaining_geohashes[i:i + CHUNK_SIZE]
            chunk_num = i // CHUNK_SIZE + 1
            total_chunks = (len(remaining_geohashes) + CHUNK_SIZE - 1) // CHUNK_SIZE
            
            print(f"\nğŸ“¦ ãƒãƒ£ãƒ³ã‚¯ {chunk_num}/{total_chunks} ã‚’å‡¦ç†ä¸­...")
            populate_pois_chunk(supabase, chunk, geohash_to_id_map, processed_geohashes, all_geohashes)
            
            print(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk_num} å®Œäº†ã€‚é€²æ—: {len(processed_geohashes)}/{len(all_geohashes)}")
            
        # æœ€çµ‚é€²æ—ä¿å­˜
        save_progress(processed_geohashes, all_geohashes)
        print(f"\nğŸ‰ å…¨å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿: {len(processed_geohashes)}/{len(all_geohashes)}")
        
        # å®Œäº†å¾Œã«é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹ã‹ç¢ºèª
        cleanup = input("é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").lower().strip()
        if cleanup == 'y':
            clear_progress()
            print("ğŸ—‘ï¸  é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
        save_progress(processed_geohashes, all_geohashes)
        print("ğŸ’¾ é€²æ—ã¯ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚æ¬¡å›ã¯ç¶šãã‹ã‚‰å®Ÿè¡Œã§ãã¾ã™ã€‚")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        save_progress(processed_geohashes, all_geohashes)
        print("ğŸ’¾ é€²æ—ã¯ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚å•é¡Œã‚’è§£æ±ºã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
